{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes by - Kiran A Bendigeri\n",
    "Please Read 'Read me' file.\n",
    "\n",
    "Unsupervised Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Unsupervised learning subsumes all kinds of machine\n",
    "learning where there is no known output\n",
    "\n",
    " Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
    "tion of the data which might be easier for humans or other machine learning algo‐\n",
    "rithms to understand compared to the original representation of the data.\n",
    "A common application of unsupervised transformations is dimensionality reduction, which takes\n",
    "a high-dimensional representation of the data, consisting of many features, and finds\n",
    "a new way to represent this data that summarizes the essential characteristics with\n",
    "fewer features. A common application for dimensionality reduction is reduction to\n",
    "two dimensions for visualization purposes.\n",
    "Another application for unsupervised transformations is finding the parts or compo‐\n",
    "nents that “make up” the data.\n",
    "\n",
    "Clustering algorithms, on the other hand, partition data into distinct groups of similar\n",
    "items. Consider the example of uploading photos to a social media site. To allow you\n",
    "131to organize your pictures, the site might want to group together pictures that show\n",
    "the same person. However, the site doesn’t know which pictures show whom, and it\n",
    "doesn’t know how many different people appear in your photo collection. A sensible\n",
    "approach would be to extract all the faces and divide them into groups of faces that\n",
    "look similar. Hopefully, these correspond to the same person, and the images can be\n",
    "grouped together for you.'''\n",
    "\n",
    "'''Preprocessing and Scaling\n",
    " adjust the features so that the data representation is more suitable for these algorithms.\n",
    "Often, this is a simple per-feature rescaling and shift of the data. '''\n",
    "import mglearn\n",
    "mglearn.plots.plot_scaling()\n",
    "\n",
    "'''Diﬀerent Kinds of Preprocessing\n",
    "The StandardScaler in scikit-learn ensures that for each\n",
    "feature the mean is 0 and the variance is 1, bringing all features to the same magni‐\n",
    "tude. However, this scaling does not ensure any particular minimum and maximum\n",
    "values for the features. The RobustScaler works similarly to the StandardScaler in\n",
    "that it ensures statistical properties for each feature that guarantee that they are on the\n",
    "same scale. However, the RobustScaler uses the median and quartiles,1 instead of\n",
    "mean and variance. This makes the RobustScaler ignore data points that are very\n",
    "different from the rest (like measurement errors). These odd data points are also\n",
    "called outliers, and can lead to trouble for other scaling techniques.\n",
    "The MinMaxScaler, on the other hand, shifts the data such that all features are exactly\n",
    "between 0 and 1. For the two-dimensional dataset this means all of the data is con‐\n",
    "tained within the rectangle created by the x-axis between 0 and 1 and the y-axis\n",
    "between 0 and 1.\n",
    "Finally, the Normalizer does a very different kind of rescaling. It scales each data\n",
    "point such that the feature vector has a Euclidean length of 1. In other words, it\n",
    "projects a data point on the circle (or sphere, in the case of higher dimensions) with a\n",
    "radius of 1. This means every data point is scaled by a different number (by the\n",
    "inverse of its length). This normalization is often used when only the direction (or\n",
    "angle) of the data matters, not the length of the feature vector.\n",
    "\n",
    "Pre‐\n",
    "processing methods like the scalers are usually applied before applying a supervised\n",
    "machine learning algorithm. As an example, say we want to apply the kernel SVM\n",
    "(SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We\n",
    "start by loading our dataset and splitting it into a training set and a test set (we need\n",
    "separate training and test sets to evaluate the supervised model we will build after the\n",
    "preprocessing):'''\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "'''To apply the transformation that we just learned—that is, to actually scale the training\n",
    "data—we use the transform method of the scaler. The transform method is used in\n",
    "scikit-learn whenever a model returns a new representation of the data:'''\n",
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(\n",
    "X_train_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(\n",
    "X_train_scaled.max(axis=0)))\n",
    "\n",
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# print test data properties after scaling\n",
    "print(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))\n",
    "\n",
    "''' if we were to use the minimum and range\n",
    "of the test set instead:\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "# split it into training and test sets\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
    "# plot the training and test sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n",
    "c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"Original Data\")\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
    "c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
    "axes[1].set_title(\"Scaled Data\")\n",
    "# rescale the test set separately\n",
    "# so test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "c=mglearn.cm2(0), label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n",
    "marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\n",
    "axes[2].set_title(\"Improperly Scaled Data\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "\n",
    "'''SVC on the original data again for comparison:'''\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "random_state=0)\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\n",
    "\n",
    "'''Now, let’s scale the data using MinMaxScaler before fitting the SVC:'''\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scoring on the scaled test set\n",
    "print(\"Scaled test set accuracy: {:.2f}\".format(\n",
    "svm.score(X_test_scaled, y_test)))\n",
    "\n",
    "'''Dimensionality Reduction, Feature Extraction, and\n",
    "Manifold Learning'''\n",
    "'''Principal component analysis is a method that rotates the dataset in a way such that\n",
    "the rotated features are statistically uncorrelated. This rotation is often followed by\n",
    "selecting only a subset of the new features, according to how important they are for\n",
    "explaining the data.'''\n",
    "\n",
    "''' the most common applications of PCA is visualizing high-dimensional data‐\n",
    "sets.'''\n",
    "import numpy as np\n",
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]\n",
    "ax = axes.ravel()\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
    "    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n",
    "    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n",
    "    ax[i].set_title(cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "ax[0].set_xlabel(\"Feature magnitude\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\n",
    "fig.tight_layout()\n",
    "\n",
    "'''Non-negative matrix factorization is another unsupervised learning algorithm that\n",
    "aims to extract useful features. It works similarly to PCA and can also be used for\n",
    "dimensionality reduction. '''\n",
    "\n",
    "'''clustering is the task of partitioning the dataset into groups,\n",
    "called clusters. The goal is to split up the data in such a way that points within a single\n",
    "cluster are very similar and points in different clusters are different. '''\n",
    "\n",
    "'''k-means clustering is one of the simplest and most commonly used clustering algo‐\n",
    "rithms. It tries to find cluster centers that are representative of certain regions of the\n",
    "data. The algorithm alternates between two steps: assigning each data point to the\n",
    "closest cluster center, and then setting each cluster center as the mean of the data\n",
    "points that are assigned to it.'''\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "# generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state=1)\n",
    "# build the clustering model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))\n",
    "print(kmeans.predict(X))\n",
    "\n",
    "''', k-means can only cap‐\n",
    "ture relatively simple shapes. k-means also assumes that all clusters have the same\n",
    "“diameter” in some sense; it always draws the boundary between clusters to be exactly\n",
    "in the middle between the cluster centers.'''\n",
    "\n",
    "'''Agglomerative clustering refers to a collection of clustering algorithms that all build\n",
    "upon the same principles: the algorithm starts by declaring each point its own cluster,\n",
    "and then merges the two most similar clusters until some stopping criterion is satis‐\n",
    "fied. The stopping criterion implemented in scikit-learn is the number of clusters,\n",
    "so similar clusters are merged until only the specified number of clusters are left.'''\n",
    "\n",
    "'''The following three choices are implemented in scikit-learn:\n",
    "ward\n",
    "The default choice, ward picks the two clusters to merge such that the variance\n",
    "within all clusters increases the least. This often leads to clusters that are rela‐\n",
    "tively equally sized.\n",
    "average\n",
    "average linkage merges the two clusters that have the smallest average distance\n",
    "between all their points.\n",
    "complete\n",
    "complete linkage (also known as maximum linkage) merges the two clusters that\n",
    "have the smallest maximum distance between their points.'''\n",
    "\n",
    "'''Another very useful clustering algorithm is DBSCAN (which stands for “densitybased spatial clustering of applications with noise”). The main benefits of DBSCAN\n",
    "are that it does not require the user to set the number of clusters a priori, it can cap‐\n",
    "ture clusters of complex shapes, and it can identify points that are not part of any\n",
    "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
    "still scales to relatively large datasets.'''\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Cluster memberships:\\n{}\".format(clusters))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
